{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5810d98c-6d20-4ab3-80ff-8264f8ffbfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d56f057b-e63a-47ac-a099-d885c5c6dd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(64*7*7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fffe564-a886-45e2-9ff0-e8be8f00a6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models with different defense methods...\n",
      "\n",
      " === Training using: standard === \n",
      "\n",
      "Epoch 1/3: Clean acc: 97.73 | PGD acc: 82.03 | FGSM acc: 4.22 | Loss: 0.255\n",
      "Epoch 2/3: Clean acc: 98.5 | PGD acc: 85.44 | FGSM acc: 9.81 | Loss: 0.06\n",
      "Epoch 3/3: Clean acc: 98.94 | PGD acc: 85.75 | FGSM acc: 6.11 | Loss: 0.042\n",
      "\n",
      " === Training using: fgsm === \n",
      "\n",
      "Epoch 1/3: Clean acc: 97.25 | PGD acc: 88.82 | FGSM acc: 71.66 | Loss: 0.817\n",
      "Epoch 2/3: Clean acc: 97.88 | PGD acc: 84.52 | FGSM acc: 80.51 | Loss: 0.394\n",
      "Epoch 3/3: Clean acc: 98.61 | PGD acc: 75.61 | FGSM acc: 84.76 | Loss: 0.268\n",
      "\n",
      " === Training using: pgd === \n",
      "\n",
      "Epoch 1/3: Clean acc: 97.64 | PGD acc: 93.86 | FGSM acc: 43.66 | Loss: 0.519\n",
      "Epoch 2/3: Clean acc: 98.22 | PGD acc: 94.92 | FGSM acc: 47.01 | Loss: 0.188\n",
      "Epoch 3/3: Clean acc: 98.54 | PGD acc: 95.96 | FGSM acc: 51.8 | Loss: 0.146\n",
      "\n",
      " === Training using: trades === \n",
      "\n",
      "Epoch 1/3: Clean acc: 97.51 | PGD acc: 94.58 | FGSM acc: 59.92 | Loss: 0.519\n",
      "Epoch 2/3: Clean acc: 97.98 | PGD acc: 95.64 | FGSM acc: 63.34 | Loss: 0.218\n",
      "Epoch 3/3: Clean acc: 98.24 | PGD acc: 96.09 | FGSM acc: 68.72 | Loss: 0.174\n",
      "\n",
      " FINAL RESULTS \n",
      "\n",
      "STANDARD: clean accuracy: 98.94, pgd_accuracy: 85.75, fgsm_acc: 6.11, robustness_gap: 13.19\n",
      "FGSM: clean accuracy: 98.61, pgd_accuracy: 75.61, fgsm_acc: 84.76, robustness_gap: 23.0\n",
      "PGD: clean accuracy: 98.54, pgd_accuracy: 95.96, fgsm_acc: 51.8, robustness_gap: 2.58\n",
      "TRADES: clean accuracy: 98.24, pgd_accuracy: 96.09, fgsm_acc: 68.72, robustness_gap: 2.15\n"
     ]
    }
   ],
   "source": [
    "def fgsm_attack(model, x, y, eps=0.3):\n",
    "    x_adv = x.clone().detach().requires_grad_(True)\n",
    "    loss = F.cross_entropy(model(x_adv), y)\n",
    "    loss.backward()\n",
    "    x_adv = x_adv + eps * x_adv.grad.sign()\n",
    "    x_adv = torch.clamp(x_adv, 0, 1)\n",
    "    return x_adv.detach()\n",
    "\n",
    "def pgd_attack(model, x, y, eps=0.3, step_size=0.01, num_steps=10, random_start=True):\n",
    "    model.eval()\n",
    "    x_adv = x.clone().detach()\n",
    "    if random_start == True:\n",
    "        x_adv = x_adv + torch.empty_like(x_adv).uniform_(-eps, eps)\n",
    "        x_adv = torch.clamp(x_adv, 0, 1)\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        x_adv.requires_grad_(True)\n",
    "        loss = F.cross_entropy(model(x_adv), y)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        grad = x_adv.grad\n",
    "        with torch.no_grad():\n",
    "            x_adv = x_adv + step_size * torch.sign(grad)\n",
    "            delta = torch.clamp(x_adv - x, -eps, eps)\n",
    "            x_adv = torch.clamp(x + delta, 0, 1).detach()\n",
    "    return x_adv\n",
    "\n",
    "def standard_training(model, optimizer, data, target):\n",
    "    optimizer.zero_grad()\n",
    "    loss = F.cross_entropy(model(data), target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def fgsm_training(model, optimizer, data, target, eps=0.3):\n",
    "    optimizer.zero_grad()\n",
    "    x_adv = fgsm_attack(model, data, target, eps)\n",
    "    alpha = 0.5\n",
    "    loss = alpha * F.cross_entropy(model(data), target) + (1-alpha) * F.cross_entropy(model(x_adv), target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def pgd_training(model, optimizer, data, target, eps=0.3, step_size=0.01, num_steps=5):\n",
    "    optimizer.zero_grad()\n",
    "    x_adv = pgd_attack(model, data, target, eps, step_size, num_steps)\n",
    "    loss = F.cross_entropy(model(x_adv), target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def trades_training(model, optimizer, data, target, beta=6.0, eps=0.3, step_size=0.01, num_steps=5):\n",
    "    optimizer.zero_grad()\n",
    "    logits_nat = model(data)\n",
    "    natural_loss = F.cross_entropy(logits_nat, target)\n",
    "    x_adv = pgd_attack(model, data, target, eps, step_size, num_steps)\n",
    "    logits_adv = model(x_adv)\n",
    "    robust_loss = F.kl_div(F.log_softmax(logits_adv, dim=1), \n",
    "                           F.softmax(logits_nat, dim=1), reduction='batchmean')\n",
    "    total_loss = natural_loss + beta * robust_loss\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    return total_loss.item(), natural_loss.item(), robust_loss.item()\n",
    "\n",
    "def evaluate_model(model, test_loader, attack_method=None, eps=0.3):\n",
    "    model.eval()\n",
    "    clean_correct, adv_correct, total = 0, 0, 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        total += target.size(0)\n",
    "        # Standard accuracy\n",
    "        with torch.no_grad():\n",
    "            _, prediction = torch.max(model(data).data, 1)\n",
    "            clean_correct += (prediction == target).sum().item()\n",
    "        # Adversarial accuracy\n",
    "        if attack_method == 'fgsm':\n",
    "            adv_data = fgsm_attack(model, data, target, eps)\n",
    "        elif attack_method == 'pgd':\n",
    "            adv_data = pgd_attack(model, data, target, eps, step_size=0.01, num_steps=5)\n",
    "        else: \n",
    "            adv_data = data\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, predicted_adv = torch.max(model(adv_data).data, 1)\n",
    "            adv_correct += (predicted_adv == target).sum().item()\n",
    "\n",
    "    clean_acc, adv_acc = 100 * clean_correct / total, 100 * adv_correct / total\n",
    "    return clean_acc, adv_acc    \n",
    "\n",
    "def train_and_compare(num_epochs=3):\n",
    "    methods = ['standard', 'fgsm', 'pgd', 'trades']\n",
    "    results = {method: {'clean_acc': [], 'pgd_acc': [], 'fgsm_acc': [], 'loss': []} for method in methods}\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    test_loader = DataLoader(datasets.MNIST('./data', train=False, transform=transform), batch_size=256, shuffle=False)\n",
    "\n",
    "    for method in methods:\n",
    "        print(f\"\\n === Training using: {method} === \\n\")\n",
    "        model = SimpleCNN().to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            num_batches = 0\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                if method == 'standard':\n",
    "                    loss = standard_training(model, optimizer, data, target)\n",
    "                elif method == 'fgsm':\n",
    "                    loss = fgsm_training(model, optimizer, data, target)\n",
    "                elif method == 'pgd':\n",
    "                    loss = pgd_training(model, optimizer, data, target)\n",
    "                elif method == 'trades':\n",
    "                    loss, _, _ = trades_training(model, optimizer, data, target)\n",
    "                epoch_loss += loss\n",
    "                num_batches += 1\n",
    "\n",
    "            clean_acc, pgd_acc = evaluate_model(model, test_loader, 'pgd')\n",
    "            _, fgsm_acc = evaluate_model(model, test_loader, 'fgsm') \n",
    "            results[method]['clean_acc'].append(clean_acc)\n",
    "            results[method]['pgd_acc'].append(pgd_acc)\n",
    "            results[method]['fgsm_acc'].append(fgsm_acc)\n",
    "            results[method]['loss'].append(epoch_loss/num_batches)\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}: Clean acc: {np.round(clean_acc, 3)} | PGD acc: {np.round(pgd_acc, 3)} | FGSM acc: {np.round(fgsm_acc, 3)} | Loss: {np.round(epoch_loss / num_batches, 3)}')\n",
    "\n",
    "    return results, methods\n",
    "\n",
    "def compare_methods(results, methods, num_epochs):\n",
    "    print(\"\\n FINAL RESULTS \\n\")\n",
    "    for method in methods:\n",
    "        clean_acc = results[method]['clean_acc'][-1]\n",
    "        pgd_acc = results[method]['pgd_acc'][-1]\n",
    "        fgsm_acc = results[method]['fgsm_acc'][-1]\n",
    "        robustness_gap = clean_acc - pgd_acc\n",
    "        print(f\"{method.upper()}: clean accuracy: {np.round(clean_acc, 3)}, pgd_accuracy: {np.round(pgd_acc, 3)}, fgsm_acc: {np.round(fgsm_acc, 3)}, robustness_gap: {np.round(robustness_gap, 3)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_epochs = 3\n",
    "    \n",
    "    print(\"Training models with different defense methods...\")\n",
    "    results, methods = train_and_compare(num_epochs)\n",
    "    \n",
    "    # Create plots\n",
    "    compare_methods(results, methods, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
